name: "explaignn"
log_level: "INFO"
dev: False

# Construct pipeline
qu: sr
ers: rers
ha: explaignn

# Define source combinations
source_combinations:
  [
    ["kb"],
    ["text"],
    ["table"],
    ["info"],
    ["kb", "text"],
    ["kb", "table"],
    ["kb", "info"],
    ["text", "table"],
    ["text", "info"],
    ["table", "info"],
    ["kb", "text", "table", "info"]
  ]

#################################################################
#  General file paths
#################################################################
path_to_stopwords: "_data/stopwords.txt"
path_to_labels: "_data/labels.pickle"
path_to_wikipedia_mappings: "_data/wikipedia_mappings.pickle"
path_to_wikidata_mappings: "_data/wikidata_mappings.pickle"

#################################################################
#  Benchmark specific settings
#################################################################
benchmark: "convmix"
benchmark_path: "_benchmarks/convmix"

train_input_path: "train_set/train_set_ALL.json"
dev_input_path: "dev_set/dev_set_ALL.json"
test_input_path: "test_set/test_set_ALL.json"

path_to_annotated: "_intermediate_representations/convmix" # where annotated inputs come from
path_to_intermediate_results: "_intermediate_representations/convmix"

#################################################################
#  Parameters - CLOCQ
#################################################################
clocq_params:
  h_match: 0.4
  h_rel: 0.2
  h_conn: 0.3
  h_coh: 0.1
  d: 20
  k: "AUTO"
  p_setting: 1000 # setting for search_space function
  bm25_limit: False
clocq_p: 1000 #  setting for neighborhood function(s) 
clocq_use_api: True # using CLOCQClientInterface
clocq_host: "https://clocq.mpi-inf.mpg.de/api" # host for client
clocq_port: "443" # port for client

#################################################################
#  Parameters - Silver annotation
#################################################################
# general
ds_sources: ["kb"]
ds_prune_noisy_qa_pairs: False

# annotation - SR
sr_relation_shared_active: False
sr_remove_stopwords: False

# OPTIONAL: annotation - turn relevance 
tr_transitive_relevances: False
tr_extract_dataset: False

#################################################################
#  Parameters - QU
#################################################################
sr_architecture: BART
sr_model_path: "_data/convmix/explaignn/sr_model.bin"
sr_max_input_length: 512

history_separator: " ||| "
sr_delimiter: "||"

sr_avoid_hallucination: True
sr_k: 10

# training parameters
sr_num_train_epochs: 5
sr_per_device_train_batch_size: 10
sr_per_device_eval_batch_size: 10
sr_warmup_steps: 500
sr_weight_decay: 0.01

# generation parameters
sr_no_repeat_ngram_size: 2
sr_num_beams: 20
sr_early_stopping: True
sr_max_output_length: 30

#################################################################
#  Parameters - ERS
#################################################################  
# cache path
ers_use_cache: True
ers_cache: "_data/convmix/explaignn/er_cache.pickle"
ers_wikipedia_dump: "_data/convmix/wikipedia_dump.pickle"
ers_wikipedia_to_wikidata_links_cache: "_data/convmix/cache_wikipedia_to_wikidata_links.pickle"
ers_on_the_fly: True

# evidence retrieval
evr_min_evidence_length: 3
evr_max_evidence_length: 200
evr_max_entities: 10 # max entities per evidence

# evidence scoring
evs_max_evidences: 500

#################################################################
#  Parameters - HA
#################################################################
# general
ha_max_answers: 50
ha_max_supporting_evidences: 5

# encoder
gnn_encoder_lm: DistilRoBERTa
gnn_encoder_linear: False
gnn_emb_dimension: 768
gnn_enc_sr_max_input: 30
gnn_enc_ev_max_input: 80
gnn_enc_ent_max_input: 60

# gnn
gnn_model: heterogeneous_gnn
gnn_num_layers: 3


gnn_answering: multitask_bilinear
gnn_max_output_evidences: 5

# dataloader
gnn_shuffle_evidences: True # shuffle evidences (no order retained)
gnn_mask_question_entities: False # avoid predicting question entities as answers
gnn_max_evidences: 500
gnn_max_entities: 1000

# training
gnn_train_max_pos_evidences: 10
gnn_train_batch_size: 1
gnn_epochs: 5
gnn_learning_rate: 0.00001
gnn_weight_decay: 0.01
gnn_clipping_max_norm: 1.0
gnn_dropout: 0.0

gnn_train:
  # GNN model for pruning
  - gnn_encoder: alternating_encoder_cross_SR
    gnn_model_path: "_data/convmix/explaignn/gnn/gnn-pruning.bin"
    gnn_multitask_answer_weight: 0.3
    gnn_multitask_ev_weight: 0.7
    gnn_decisive_metric: answer_presence # metric for choosing best model from dev set

    gnn_max_evidences: 500
    gnn_max_entities: 1000

  # GNN model for answering
  - gnn_encoder: full_encoder_cross_SR
    gnn_add_entity_type: True
    gnn_model_path: "_data/convmix/explaignn/gnn/gnn-answering.bin"
    gnn_multitask_answer_weight: 0.5
    gnn_multitask_ev_weight: 0.5
    gnn_decisive_metric: p_at_1 # metric for choosing best model from dev set

    gnn_max_evidences: 500
    gnn_max_entities: 1000

# inference/eval
gnn_eval_batch_size: 10
gnn_inference:
  - gnn_model_path: "_data/convmix/explaignn/gnn/gnn-pruning.bin"
    gnn_encoder: alternating_encoder_cross_SR
    gnn_max_evidences: 500
    gnn_max_entities: 1000
    gnn_max_output_evidences: 100

  - gnn_encoder: alternating_encoder_cross_SR
    gnn_model_path: "_data/convmix/explaignn/gnn/gnn-pruning.bin"
    gnn_max_evidences: 100
    gnn_max_entities: 400
    gnn_max_output_evidences: 20

  - gnn_encoder: full_encoder_cross_SR
    gnn_add_entity_type: True
    gnn_model_path: "_data/convmix/explaignn/gnn/gnn-answering.bin"
    gnn_max_evidences: 20
    gnn_max_entities: 80
    gnn_max_output_evidences: 5